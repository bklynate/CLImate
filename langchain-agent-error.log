{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:15:48.634Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-12T04:15:48.634Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:16:06.887Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-12T04:16:06.888Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:20:44.365Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-12T04:20:44.365Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:47:07.830Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-12T04:47:07.830Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:47:13.802Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-12T04:47:13.803Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:33:39.105Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-13T03:33:39.106Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:34:56.826Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-13T03:34:56.828Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:35:26.220Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-13T03:35:26.221Z"}
{"error":"400 No models loaded. Please load a model in the developer page or use the 'lms load' command.","level":"error","message":"[cli] Agent error","stack":"Error: 400 No models loaded. Please load a model in the developer page or use the 'lms load' command.\n    at Function.generate (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/error.ts:72:14)\n    at OpenAI.makeStatusError (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/core.ts:462:21)\n    at OpenAI.makeRequest (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/core.ts:526:24)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/openai/dist/chat_models.js:2022:29\n    at async RetryOperation._fn (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/p-retry/index.js:50:12)","timestamp":"2026-01-13T12:27:00.020Z"}
{"error":"Reached context length of 4096 tokens, but this model does not currently support mid-generation context overflow because arch is gpt-oss. Try reloading with a larger context length or shortening the prompt/chat.","level":"error","message":"[cli] Agent error","stack":"Error: Reached context length of 4096 tokens, but this model does not currently support mid-generation context overflow because arch is gpt-oss. Try reloading with a larger context length or shortening the prompt/chat.\n    at Stream.iterator (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/streaming.ts:76:21)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async ChatOpenAI._streamResponseChunks (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/openai/dist/chat_models.js:1647:26)\n    at async ChatOpenAI._generateUncached (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/core/dist/language_models/chat_models.js:211:34)\n    at async ChatOpenAI.invoke (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/core/dist/language_models/chat_models.js:88:24)\n    at async RunnableSequence.invoke (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/core/dist/runnables/base.js:1314:27)\n    at async RunnableCallable.callModel (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/langgraph/src/prebuilt/react_agent_executor.ts:593:23)\n    at async RunnableCallable.invoke (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/langgraph/src/utils.ts:85:21)\n    at async RunnableSequence.invoke (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/core/dist/runnables/base.js:1308:33)\n    at async _runWithRetry (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/langgraph/src/pregel/retry.ts:102:16)","timestamp":"2026-01-16T06:14:13.757Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-17T17:16:40.480Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-17T17:16:40.481Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-23T02:01:32.241Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LOG_LEVEL: Invalid enum value. Expected 'error' | 'warn' | 'info' | 'debug' | 'verbose', received 'invalid'","timestamp":"2026-01-23T02:01:32.242Z"}
