{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:15:48.618Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:15:48.634Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:16:06.872Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:16:06.887Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:20:44.349Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:20:44.365Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-12T04:43:34.810Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-12T04:47:07.271Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:47:07.814Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:47:07.830Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:47:13.787Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:47:13.802Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-12T04:47:34.185Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-12T05:08:20.952Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-13T03:33:39.077Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:33:39.105Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-13T03:34:56.810Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:34:56.826Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-13T03:35:26.202Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:35:26.219Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-13T04:54:28.637Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-13T12:22:49.516Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-13T12:26:22.120Z"}
{"error":"400 No models loaded. Please load a model in the developer page or use the 'lms load' command.","level":"error","message":"[cli] Agent error","stack":"Error: 400 No models loaded. Please load a model in the developer page or use the 'lms load' command.\n    at Function.generate (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/error.ts:72:14)\n    at OpenAI.makeStatusError (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/core.ts:462:21)\n    at OpenAI.makeRequest (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/core.ts:526:24)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/openai/dist/chat_models.js:2022:29\n    at async RetryOperation._fn (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/p-retry/index.js:50:12)","timestamp":"2026-01-13T12:27:00.020Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-13T12:27:00.021Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-14T04:37:25.421Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-14T05:23:08.135Z"}
