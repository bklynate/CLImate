{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:15:48.618Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:15:48.634Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:16:06.872Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:16:06.887Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:20:44.349Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:20:44.365Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-12T04:43:34.810Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-12T04:47:07.271Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:47:07.814Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:47:07.830Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-12T04:47:13.787Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-12T04:47:13.802Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-12T04:47:34.185Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-12T05:08:20.952Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-13T03:33:39.077Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:33:39.105Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-13T03:34:56.810Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:34:56.826Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-13T03:35:26.202Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-13T03:35:26.219Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-13T04:54:28.637Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-13T12:22:49.516Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-13T12:26:22.120Z"}
{"error":"400 No models loaded. Please load a model in the developer page or use the 'lms load' command.","level":"error","message":"[cli] Agent error","stack":"Error: 400 No models loaded. Please load a model in the developer page or use the 'lms load' command.\n    at Function.generate (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/error.ts:72:14)\n    at OpenAI.makeStatusError (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/core.ts:462:21)\n    at OpenAI.makeRequest (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/core.ts:526:24)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/openai/dist/chat_models.js:2022:29\n    at async RetryOperation._fn (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/p-retry/index.js:50:12)","timestamp":"2026-01-13T12:27:00.020Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-13T12:27:00.021Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-14T04:37:25.421Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-14T05:23:08.135Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-14T05:35:27.194Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-14T05:38:42.189Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-14T05:38:51.256Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-14T05:44:20.312Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-14T06:02:18.222Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-14T06:05:32.507Z"}
{"level":"info","message":"[cli] CLI started","streaming":true,"threadId":"cff4b88f-b64c-4281-b1af-f5dece637938","timestamp":"2026-01-16T06:10:07.901Z"}
{"error":"Reached context length of 4096 tokens, but this model does not currently support mid-generation context overflow because arch is gpt-oss. Try reloading with a larger context length or shortening the prompt/chat.","level":"error","message":"[cli] Agent error","stack":"Error: Reached context length of 4096 tokens, but this model does not currently support mid-generation context overflow because arch is gpt-oss. Try reloading with a larger context length or shortening the prompt/chat.\n    at Stream.iterator (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/openai/src/streaming.ts:76:21)\n    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\n    at async ChatOpenAI._streamResponseChunks (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/openai/dist/chat_models.js:1647:26)\n    at async ChatOpenAI._generateUncached (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/core/dist/language_models/chat_models.js:211:34)\n    at async ChatOpenAI.invoke (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/core/dist/language_models/chat_models.js:88:24)\n    at async RunnableSequence.invoke (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/core/dist/runnables/base.js:1314:27)\n    at async RunnableCallable.callModel (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/langgraph/src/prebuilt/react_agent_executor.ts:593:23)\n    at async RunnableCallable.invoke (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/langgraph/src/utils.ts:85:21)\n    at async RunnableSequence.invoke (file:///Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/core/dist/runnables/base.js:1308:33)\n    at async _runWithRetry (/Users/bklynate/dev_work/aiWorld/agent-from-scratch/node_modules/@langchain/langgraph/src/pregel/retry.ts:102:16)","timestamp":"2026-01-16T06:14:13.757Z"}
{"level":"info","message":"[cli] User force closed","timestamp":"2026-01-17T17:12:03.313Z"}
{"level":"debug","message":"[config] Configuration loaded successfully","model":"gpt-4o","provider":"openai","timestamp":"2026-01-17T17:16:40.464Z"}
{"level":"error","message":"[config] Configuration validation failed:\n  - LLM_PROVIDER: Invalid enum value. Expected 'ollama' | 'lmstudio' | 'lm-studio' | 'openai', received 'invalid-provider'","timestamp":"2026-01-17T17:16:40.480Z"}
